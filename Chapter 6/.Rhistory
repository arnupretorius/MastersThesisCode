BayesClasses <- as.numeric(factor(apply(simTest[,-31], 1, function(x) 1*(0.5<(q+(1-2*q)*1*(sum((x[1:J]))>(J/2)))))))
setup5Results <- runBiasVarSimulation(trainingSets, simTest, setup5ParaGrid, BayesClasses)
saveRDS(setup5Results, "obliqueRRFsetup5ResultsAR.rda")
# ----------------------------------------------------------------------------------------
####################################################################################
# J = 5
####################################################################################
# ----------------------------------------------------------------------------------------
J <- 5
# simluating training data sets
simData1 <- generateMeasedata(J=5)
trainingSets <- simData1[[1]]
simTest <- simData1[[2]]
# run simulation and plot data
setup6TuneVals <- readRDS("setup6ResultsAR.rda")[[2]]
setup6ParaGrid <- c(median(setup6TuneVals[[3]][,2]), median(setup6TuneVals[[4]]))
BayesClasses <- as.numeric(factor(apply(simTest[,-31], 1, function(x) 1*(0.5<(q+(1-2*q)*1*(sum((x[1:J]))>(J/2)))))))
setup6Results <- runBiasVarSimulation(trainingSets, simTest, setup6ParaGrid, BayesClasses)
saveRDS(setup6Results, "obliqueRRFsetup6ResultsAR.rda")
# ----------------------------------------------------------------------------------------
####################################################################################
# J = 15
####################################################################################
# ----------------------------------------------------------------------------------------
J <- 15
# simluating training data sets
simData1 <- generateMeasedata(J=15)
trainingSets <- simData1[[1]]
simTest <- simData1[[2]]
# run simulation and plot data
setup7TuneVals <- readRDS("setup7ResultsAR.rda")[[2]]
setup7ParaGrid <- c(median(setup7TuneVals[[3]][,2]), median(setup7TuneVals[[4]]))
BayesClasses <- as.numeric(factor(apply(simTest[,-31], 1, function(x) 1*(0.5<(q+(1-2*q)*1*(sum((x[1:J]))>(J/2)))))))
setup7Results <- runBiasVarSimulation(trainingSets, simTest, setup7ParaGrid, BayesClasses)
saveRDS(setup7Results, "obliqueRRFsetup7ResultsAR.rda")
# ----------------------------------------------------------------------------------------
####################################################################################
# J = 20
####################################################################################
# ----------------------------------------------------------------------------------------
J <- 20
# simluating training data sets
simData1 <- generateMeasedata(J=20)
trainingSets <- simData1[[1]]
simTest <- simData1[[2]]
# run simulation and plot data
setup8TuneVals <- readRDS("setup8ResultsAR.rda")[[2]]
setup8ParaGrid <- c(median(setup8TuneVals[[3]][,2]), median(setup8TuneVals[[4]]))
BayesClasses <- as.numeric(factor(apply(simTest[,-31], 1, function(x) 1*(0.5<(q+(1-2*q)*1*(sum((x[1:J]))>(J/2)))))))
setup8Results <- runBiasVarSimulation(trainingSets, simTest, setup8ParaGrid, BayesClasses)
saveRDS(setup8Results, "obliqueRRFsetup8ResultsAR.rda")
twonormResults
threenormResults
ringnormResults
setup5Results
setup6Results
setup7Results
setup8Results
res1 <- readRDS("twonormResultsAR.rda")
res2 <- list(results=readRDS("threenormResultsAR.rda"))
res3 <- readRDS("ringnormResultsAR.rda")
res4 <- readRDS("circleResultsAR.rda")
res5 <- readRDS("setup1ResultsAR.rda")
res6 <- readRDS("setup2ResultsAR.rda")
res7 <- readRDS("setup3ResultsAR.rda")
res8 <- readRDS("setup4ResultsAR.rda")
res9 <- readRDS("setup5ResultsAR.rda")
res10 <- readRDS("setup6ResultsAR.rda")
res11 <- readRDS("setup7ResultsAR.rda")
res12 <- readRDS("setup8ResultsAR.rda")
resList <- list(res1, res2, res3, res4, res5, res6, res7, res8,
res9, res10, res11, res12)
tableFinal <- NULL
for(k in 1:length(resList)){
res <- resList[[k]]
splitDat <- split(res$results, res$results$model)
cname <- unique(res$results$model)
rname <- unique(res$results$Decomposition)
tableFrame <- matrix(0, nrow=length(rname), ncol=length(cname))
for(i in 1:length(splitDat)){
tableFrame[,i] <- splitDat[[i]]$vb
}
rownames(tableFrame) <- paste(k, rname)
colnames(tableFrame) <- cname
tableFinal <- rbind(tableFinal, tableFrame)
}
tableFinal <- as.data.frame(tableFinal)
# create latex table
library(stargazer)
stargazer(tableFinal, summary = FALSE)
y
rm(y)
simulateBiasVarDecomp <- function(trainingSets, simTest, model, paraGrid, BayesPreds, ...){
#################################################
# BIAS VARIANCE EXPERMIMENT USING MLBENCH DATA  #
#################################################
# ----------------------------------------------------------------------------------------
library(mlbench)
library(caret)
library(ggplot2)
majVote <- function(x){names(which.max(table(x)))}
numOfExp <- 1
# ----------------------------------------------------------------------------------------
#######################################################
# Bias and variance for single tree model 2d normals
#######################################################
# train models and make predictions
BVpreds <- matrix(0, nrow=numOfExp, ncol=nTest)
var.T <- NULL
var <- NULL
bias <- NULL
VE <- NULL
SE <- NULL
misclassError <- NULL
p <- ncol(simTest)
C <- as.numeric(simTest$classes)
# train models
for(j in 1:numOfExp){
x <- trainingSets[[j]][,-p]
y <- trainingSets[[j]][,p]
mod <- obliqueRRF(x=x, y=y, K=paraGrid[1], L=200, mtry=paraGrid[2], model=model)
BVpreds[j,] <- predict.obliqueRRF(mod, simTest[,-p])+1
print(paste("Method: ", model, ", Iter: ", j, " out of ", numOfExp))
}
# James (2003) decomposition estimates
BayesClassifier <- BayesPreds
majVoteClassifier <- apply(BVpreds, 2, function(x)majVote(x))
var.T <- mean(BayesClassifier != C)
var <- mean(apply(BVpreds, 1, function(x) mean(x != majVoteClassifier)))
bias <- mean(majVoteClassifier != BayesClassifier)
VE <- mean(apply(BVpreds, 1, function(x) mean(x != C)) - mean(majVoteClassifier != C))
SE <- mean(majVoteClassifier != C) - mean(BayesClassifier != C)
meanError <- mean(apply(BVpreds, 1, function(x){ mean(x != C) }))
# plot bias and variance and systematic effect and variance effect
vb <- c(meanError, var.T, SE, VE, bias, var)
bar <- factor(c(1,2,3,4,5,6))
type <- c("Error", "Bayes Error", "Systematic Effect", "Variance Effect", "Bias", "Variance")
modelName <- rep(model, 6)
biasVarPlotData <- data.frame(vb=vb, Decomposition=type, bar=bar, model=modelName)
biasVarPlotData
}
trainingSets <- list()
for(i in 1:100){
set.seed(i+1)
train <- simData(nvars=c(15), cors=c(0.9), associations=c(1),
firstonly=c(FALSE), nsamples=400, response="binary")
train <- train$data
train$classes <- train$outcome
trainingSets[[i]] <- train[,-16]
}
# simulate test data set
set.seed(1)
test <- simData(nvars=c(15), cors=c(0.9), associations=c(1),
firstonly=c(FALSE), nsamples=1000, response="binary")
testData <- test$data
testData$classes <- testData$outcome
simTest <- testData[,-16]
# run simulation and plot data
setup1TuneVals <- readRDS("setup1ResultsAR.rda")[[2]]
setup1ParaGrid <- c(median(setup1TuneVals[[3]][,2]), median(setup1TuneVals[[4]]))
BayesClasses <- as.numeric(factor(ifelse(test$probs > 0.5, 1, 0)))
setup1Results <- runBiasVarSimulation(trainingSets, simTest, setup1ParaGrid, BayesClasses)
?predict.oblique.tree
predict.obliqueRRF <- function (object, newdata, type="class"){
predType <- ifelse(class(object$models[[1]])[1] == "oblique.tree", "vector", "prob")
newdata <- data.frame(sapply(newdata, as.numeric))
if (!identical(colnames(newdata), object$columnnames))
stop("Variable names and/or order of variables in newdata is not identical to training set. Please check if variables are exactly the same in both sets.")
predicted <- matrix(NA, nrow = nrow(newdata), ncol = length(object$models))
for (i in 1:length(object$models)) {
final <- data.frame(as.matrix(newdata) %*% as.matrix(object$loadings[[i]]))
predicted[, i] <- predict(object$models[[i]], newdata,
type = predType)[, 2]
}
if (type=="class") {
ifelse(rowMeans(predicted) > 0.5, 1, 0)
}
else if(type=="prob") {
rowMeans(predicted)
} else {
stop("Argument 'type' must be either 'class' or 'prob'")
}
}
trainingSets <- list()
for(i in 1:100){
set.seed(i+1)
train <- simData(nvars=c(15), cors=c(0.9), associations=c(1),
firstonly=c(FALSE), nsamples=400, response="binary")
train <- train$data
train$classes <- train$outcome
trainingSets[[i]] <- train[,-16]
}
# simulate test data set
set.seed(1)
test <- simData(nvars=c(15), cors=c(0.9), associations=c(1),
firstonly=c(FALSE), nsamples=1000, response="binary")
testData <- test$data
testData$classes <- testData$outcome
simTest <- testData[,-16]
# run simulation and plot data
setup1TuneVals <- readRDS("setup1ResultsAR.rda")[[2]]
setup1ParaGrid <- c(median(setup1TuneVals[[3]][,2]), median(setup1TuneVals[[4]]))
BayesClasses <- as.numeric(factor(ifelse(test$probs > 0.5, 1, 0)))
setup1Results <- runBiasVarSimulation(trainingSets, simTest, setup1ParaGrid, BayesClasses)
predict.obliqueRRF <- function (object, newdata, type="class"){
predType <- ifelse(class(object$models[[1]])[1] == "oblique.tree", "vector", "prob")
newdata <- data.frame(sapply(newdata, as.numeric))
if (!identical(colnames(newdata), object$columnnames))
stop("Variable names and/or order of variables in newdata is not identical to training set. Please check if variables are exactly the same in both sets.")
predicted <- matrix(NA, nrow = nrow(newdata), ncol = length(object$models))
for (i in 1:length(object$models)) {
final <- data.frame(as.matrix(newdata) %*% as.matrix(object$loadings[[i]]))
predicted[, i] <- predict(object$models[[i]], final,
type = predType)[, 2]
}
if (type=="class") {
ifelse(rowMeans(predicted) > 0.5, 1, 0)
}
else if(type=="prob") {
rowMeans(predicted)
} else {
stop("Argument 'type' must be either 'class' or 'prob'")
}
}
trainingSets <- list()
for(i in 1:100){
set.seed(i+1)
train <- mlbench.twonorm(400, d=20)
train <- as.data.frame(train)
trainingSets[[i]] <- train
}
# simulate test data set
set.seed(1)
test <- mlbench.twonorm(1000, d=20)
testFrame <- as.data.frame(test)
simTest <- testFrame
# run simulation and plot data
twonormTuneVals <- readRDS("twonormResultsAR.rda")[[2]]
twonormParaGrid <- c(median(twonormTuneVals[[3]][,2]), median(twonormTuneVals[[4]]))
twonormResults <- runBiasVarSimulation(trainingSets, simTest, twonormParaGrid, bayesclass(test))
?obliqueRF
?oblique.tree
obliqueRRF <- function (x, y, K = round(ncol(x)/3, 0), L = 10, mtry=floor(sqrt(ncol(x))), model="linComb", ...){
require(randomForest)
require(oblique.tree)
require(obliqueRF)
#x <- data.frame(sapply(x, as.numeric))
x <- model.matrix(~., x)[,-1]
y <- factor(as.numeric(y)-1)
while (ncol(x)%%K != 0) {
K <- K - 1
}
M <- round(ncol(x)/K)
predicted <- list()
fit <- numeric()
Ri <- list()
Ria <- list()
fit <- list()
predicted <- matrix(NA, nrow = nrow(x), ncol = L)
subsets <- list()
SelectedClass <- list()
IndependentsClassSubset <- list()
IndependentsClassSubsetBoot <- list()
pcdata <- list()
loadings <- list()
for (i in 1:L) {
Independents <- x[, sample(1:ncol(x), ncol(x))]
n <- 0
subsets[[i]] <- list()
SelectedClass[[i]] <- list()
IndependentsClassSubset[[i]] <- list()
IndependentsClassSubsetBoot[[i]] <- list()
pcdata[[i]] <- list()
loadings[[i]] <- list()
for (j in seq(1, K)) {
n <- n + M
subsets[[i]][[j]] <- data.frame(Independents[, (n -
(M - 1)):n], y)
SelectedClass[[i]][[j]] <- as.integer(sample(levels(as.factor(y)),
1))
IndependentsClassSubset[[i]][[j]] <- subsets[[i]][[j]][subsets[[i]][[j]]$y ==
SelectedClass[[i]][[j]], ]
IndependentsClassSubsetBoot[[i]][[j]] <- IndependentsClassSubset[[i]][[j]][sample(1:dim(IndependentsClassSubset[[i]][[j]])[1],
round(0.75 * nrow(IndependentsClassSubset[[i]][[j]])),
replace = TRUE), ]
pcdata[[i]][[j]] <- princomp(IndependentsClassSubsetBoot[[i]][[j]][,
!colnames(IndependentsClassSubsetBoot[[i]][[j]]) %in%
"y"])
loadings[[i]][[j]] <- pcdata[[i]][[j]]$loadings[,
]
colnames(loadings[[i]][[j]]) <- dimnames(loadings[[i]][[j]])[[1]]
loadings[[i]][[j]] <- data.frame(dimnames(loadings[[i]][[j]])[[1]],
loadings[[i]][[j]])
colnames(loadings[[i]][[j]])[1] <- "rowID"
}
Ri[[i]] <- Reduce(function(x, y) merge(x, y, by = "rowID",
all = TRUE), loadings[[i]])
Ri[[i]][is.na(Ri[[i]])] <- 0
Ria[[i]] <- Ri[[i]][order(match(Ri[[i]]$rowID, colnames(x))),
order(match(colnames(Ri[[i]]), colnames(x)))]
rownames(Ria[[i]]) <- Ria[[i]]$rowID
Ria[[i]]$rowID <- NULL
finalx <- data.frame(as.matrix(x) %*% as.matrix(Ria[[i]]))
final <- data.frame(finalx, y)
if(model=="linComb"){
fit[[i]] <- oblique.tree(y ~ ., data = final,
...)
} else if(model=="rf"){
fit[[i]] <- randomForest(y ~ ., data = final, mtry=mtry, ntree=1,
...)
} else if(model %in% c("ridge", "pls", "log", "svm", "rnd")){
capture.output(fit[[i]] <- obliqueRF(x = as.matrix(finalx), y=as.numeric(y), mtry=mtry, ntree=1,
training_method=model, verbose = FALSE, ...))
} else {
stop("Argument 'model' not a valid model type.")
}
}
res <- list(models = fit, loadings = Ria, columnnames = colnames(x))
class(res) <- "obliqueRotationForest"
res
}
trainingSets <- list()
for(i in 1:100){
set.seed(i+1)
train <- mlbench.twonorm(400, d=20)
train <- as.data.frame(train)
trainingSets[[i]] <- train
}
# simulate test data set
set.seed(1)
test <- mlbench.twonorm(1000, d=20)
testFrame <- as.data.frame(test)
simTest <- testFrame
# run simulation and plot data
twonormTuneVals <- readRDS("twonormResultsAR.rda")[[2]]
twonormParaGrid <- c(median(twonormTuneVals[[3]][,2]), median(twonormTuneVals[[4]]))
twonormResults <- runBiasVarSimulation(trainingSets, simTest, twonormParaGrid, bayesclass(test))
obliqueRRF <- function (x, y, K = round(ncol(x)/3, 0), L = 10, mtry=floor(sqrt(ncol(x))), model="log", ...){
require(randomForest)
require(oblique.tree)
require(obliqueRF)
#x <- data.frame(sapply(x, as.numeric))
x <- model.matrix(~., x)[,-1]
y <- factor(as.numeric(y)-1)
while (ncol(x)%%K != 0) {
K <- K - 1
}
M <- round(ncol(x)/K)
predicted <- list()
fit <- numeric()
Ri <- list()
Ria <- list()
fit <- list()
predicted <- matrix(NA, nrow = nrow(x), ncol = L)
subsets <- list()
SelectedClass <- list()
IndependentsClassSubset <- list()
IndependentsClassSubsetBoot <- list()
pcdata <- list()
loadings <- list()
for (i in 1:L) {
Independents <- x[, sample(1:ncol(x), ncol(x))]
n <- 0
subsets[[i]] <- list()
SelectedClass[[i]] <- list()
IndependentsClassSubset[[i]] <- list()
IndependentsClassSubsetBoot[[i]] <- list()
pcdata[[i]] <- list()
loadings[[i]] <- list()
for (j in seq(1, K)) {
n <- n + M
subsets[[i]][[j]] <- data.frame(Independents[, (n -
(M - 1)):n], y)
SelectedClass[[i]][[j]] <- as.integer(sample(levels(as.factor(y)),
1))
IndependentsClassSubset[[i]][[j]] <- subsets[[i]][[j]][subsets[[i]][[j]]$y ==
SelectedClass[[i]][[j]], ]
IndependentsClassSubsetBoot[[i]][[j]] <- IndependentsClassSubset[[i]][[j]][sample(1:dim(IndependentsClassSubset[[i]][[j]])[1],
round(0.75 * nrow(IndependentsClassSubset[[i]][[j]])),
replace = TRUE), ]
pcdata[[i]][[j]] <- princomp(IndependentsClassSubsetBoot[[i]][[j]][,
!colnames(IndependentsClassSubsetBoot[[i]][[j]]) %in%
"y"])
loadings[[i]][[j]] <- pcdata[[i]][[j]]$loadings[,
]
colnames(loadings[[i]][[j]]) <- dimnames(loadings[[i]][[j]])[[1]]
loadings[[i]][[j]] <- data.frame(dimnames(loadings[[i]][[j]])[[1]],
loadings[[i]][[j]])
colnames(loadings[[i]][[j]])[1] <- "rowID"
}
Ri[[i]] <- Reduce(function(x, y) merge(x, y, by = "rowID",
all = TRUE), loadings[[i]])
Ri[[i]][is.na(Ri[[i]])] <- 0
Ria[[i]] <- Ri[[i]][order(match(Ri[[i]]$rowID, colnames(x))),
order(match(colnames(Ri[[i]]), colnames(x)))]
rownames(Ria[[i]]) <- Ria[[i]]$rowID
Ria[[i]]$rowID <- NULL
finalx <- data.frame(as.matrix(x) %*% as.matrix(Ria[[i]]))
final <- data.frame(finalx, y)
if(model=="rf"){
fit[[i]] <- randomForest(y ~ ., data = final, mtry=mtry, ntree=1,
...)
} else if(model %in% c("ridge", "pls", "log", "svm", "rnd")){
capture.output(fit[[i]] <- obliqueRF(x = as.matrix(finalx), y=as.numeric(y), mtry=mtry, ntree=1,
training_method=model, verbose = FALSE, ...))
} else {
stop("Argument 'model' not a valid model type.")
}
}
res <- list(models = fit, loadings = Ria, columnnames = colnames(x))
class(res) <- "obliqueRotationForest"
res
}
runBiasVarSimulation <- function(trainingSets, simTest, paraGrid, BayesPreds){
# linear combination oblique trees
sim.obliqueRRFrf <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
model="rf", paraGrid = paraGrid, BayesPreds = BayesPreds)
# linear combination oblique trees
#sim.obliqueRRFlinComb <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
#                                          model="linComb", paraGrid = paraGrid, BayesPreds = BayesPreds)
# randomised oblique trees using logistic splits
sim.obliqueRRFlog <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
model="log", paraGrid = paraGrid, BayesPreds = BayesPreds)
# rotation random forest
#sim.RRF <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
#                                 model="pls", paraGrid = rrfparaGrid,
#                                 tControl = fitControl, BayesPreds = BayesPreds)
# oblique random forest (logistic) model
#sim.ORF <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
#                                 method="svm", paraGrid = orfparaGrid,
#                                 tControl = fitControl, BayesPreds = BayesPreds, ntree=200)
# random forest model
#sim.RF <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
#                                method="log", paraGrid = rfparaGrid,
#                                tControl = fitControl, BayesPreds = BayesPreds, ntree=200)
#list(results=rbind(sim.RF$results, sim.ERF$results, sim.RRF$results, sim.ORF$results),
#     tuneValues=list(sim.RF$tuneValues, sim.ERF$tuneValues, sim.RRF$tuneValues, sim.ORF$tuneValues))
list(sim.obliqueRRFlog, sim.obliqueRRFlinComb, sim.obliqueRRFrf)
}
trainingSets <- list()
for(i in 1:100){
set.seed(i+1)
train <- mlbench.threenorm(400, d=20)
train <- as.data.frame(train)
trainingSets[[i]] <- train
}
# simulate test data set
set.seed(1)
test <- mlbench.threenorm(1000, d=20)
testFrame <- as.data.frame(test)
simTest <- testFrame
# run simulation and plot data
threenormTuneVals <- readRDS("threenormResultsAR.rda")[[2]]
threenormParaGrid <- c(5, 4)
threenormResults <- runBiasVarSimulation(trainingSets, simTest, threenormParaGrid, bayesclass(test))
runBiasVarSimulation <- function(trainingSets, simTest, paraGrid, BayesPreds){
# linear combination oblique trees
sim.obliqueRRFrf <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
model="rf", paraGrid = paraGrid, BayesPreds = BayesPreds)
# linear combination oblique trees
#sim.obliqueRRFlinComb <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
#                                          model="linComb", paraGrid = paraGrid, BayesPreds = BayesPreds)
# randomised oblique trees using logistic splits
sim.obliqueRRFlog <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
model="log", paraGrid = paraGrid, BayesPreds = BayesPreds)
# rotation random forest
#sim.RRF <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
#                                 model="pls", paraGrid = rrfparaGrid,
#                                 tControl = fitControl, BayesPreds = BayesPreds)
# oblique random forest (logistic) model
#sim.ORF <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
#                                 method="svm", paraGrid = orfparaGrid,
#                                 tControl = fitControl, BayesPreds = BayesPreds, ntree=200)
# random forest model
#sim.RF <- simulateBiasVarDecomp(trainingSets=trainingSets, simTest=simTest,
#                                method="log", paraGrid = rfparaGrid,
#                                tControl = fitControl, BayesPreds = BayesPreds, ntree=200)
#list(results=rbind(sim.RF$results, sim.ERF$results, sim.RRF$results, sim.ORF$results),
#     tuneValues=list(sim.RF$tuneValues, sim.ERF$tuneValues, sim.RRF$tuneValues, sim.ORF$tuneValues))
list(sim.obliqueRRFlog, sim.obliqueRRFrf)
}
trainingSets <- list()
for(i in 1:100){
set.seed(i+1)
train <- simData(nvars=c(15), cors=c(0.9), associations=c(1),
firstonly=c(FALSE), nsamples=400, response="binary")
train <- train$data
train$classes <- train$outcome
trainingSets[[i]] <- train[,-16]
}
# simulate test data set
set.seed(1)
test <- simData(nvars=c(15), cors=c(0.9), associations=c(1),
firstonly=c(FALSE), nsamples=1000, response="binary")
testData <- test$data
testData$classes <- testData$outcome
simTest <- testData[,-16]
# run simulation and plot data
setup1TuneVals <- readRDS("setup1ResultsAR.rda")[[2]]
setup1ParaGrid <- c(median(setup1TuneVals[[3]][,2]), median(setup1TuneVals[[4]]))
BayesClasses <- as.numeric(factor(ifelse(test$probs > 0.5, 1, 0)))
setup1Results <- runBiasVarSimulation(trainingSets, simTest, setup1ParaGrid, BayesClasses)
threenormResults
twonormResults
ringnormResults
setup1Results
setup5Results
setup6Results
setup7Results
setup8Results
ringnormResults
setup5Results
