proby <- TeX("$P(Y|x)$")
probtry <- TeX("$P_{\\Omega_{tr}}(Y|x)$")
bayes <- TeX("$Irreducible Error$")
var <- TeX("$Variance$")
bias <- TeX("$Bias$")
bayesModel <- TeX("$f_B(x)$")
avgModel <- TeX("$\\bar{f}(x)$")
text(30, 0.005,proby,cex=0.8)
text(150, 0.005, probtry, cex=0.8)
text(35, max(hx1), bayes, cex=0.8)
text(135, max(hx2), var, cex=0.8)
text((mean1+mean2)/2, 0.032, bias, cex=0.8)
text(mean1, -0.0029, bayesModel, cex=0.8)
text(mean2, -0.0029, avgModel, cex=0.8)
text(-6, 0.038, "P")
text(190, -0.0025, "Y")
# ---------------------------------------------------------------------------
#########################################################################
# Figure 5.2: Bias and variance of an estimated distribution: Left: Large
# bias and small variance. Right: Small bias and large variance.
#########################################################################
# ---------------------------------------------------------------------------
# Large bias, small variance
# Specify means and standard deviations
mean1=85; sd1=15
lb1=80; ub1=120
mean2=140; sd2=5
lb2=80; ub2=120
# simulate data
x1 <- seq(-4,4,length=100)*sd1 + mean1
hx1 <- dnorm(x1,mean1,sd1)
x2 <- seq(-4,4,length=100)*sd2 + mean2
hx2 <- dnorm(x2,mean2,sd2)
# create empty plot
plot(x1, hx1, type="n", xlab="", ylab="",
xlim=c(0, 200), ylim=c(-0.003, 0.08),
main="", axes=FALSE)
# draw distributions
lines(x1, hx1)
lines(x2, hx2)
arrows(0, 0, 0, 0.08, xpd = TRUE, length = 0.1)
arrows(0, 0, 200, 0, xpd = TRUE, length = 0.1)
arrows(mean2-sd2, max(hx2), mean2+sd2, max(hx2), length = 0.05, code=3, col="darkorange")
arrows(mean1-sd1, max(hx1), mean1+sd1, max(hx1), length = 0.05, code=3, col="darkorange")
arrows(mean1, 0.5*max(hx2), mean2, 0.5*max(hx2), length = 0.05, code=3, col="darkgreen")
lines(c(mean1,mean1), c(0, 0.08), lty=2, col="blue")
lines(c(mean2, mean2), c(0, 0.08), lty=2, col="blue")
# add text to the plot
proby <- TeX("$P(y|x)$")
probtry <- TeX("$P_{\\Omega_{tr}}(y|x)$")
bayesModel <- TeX("$f_B(x)$")
avgModel <- TeX("$\\bar{f}(x)$")
text(65, 0.02,proby,cex=0.9)
text(165, 0.005, probtry, cex=0.9)
text((mean1+mean2)/2, 0.5*max(hx2)+0.003, "Large bias", cex=0.9)
text(mean2+sd2+17, max(hx2), "Small variance", cex=0.9)
text(mean1, -0.0029, bayesModel, cex=0.9)
text(mean2, -0.0029, avgModel, cex=0.9)
text(-5, 0.078, "P")
text(190, -0.0025, "Y")
# Small bias, large variance
# specify means and standard deviations
mean1=85; sd1=15
lb1=80; ub1=120
mean2=90; sd2=25
lb2=80; ub2=120
# simulate data
x1 <- seq(-4,4,length=100)*sd1 + mean1
hx1 <- dnorm(x1,mean1,sd1)
x2 <- seq(-3,4,length=100)*sd2 + mean2
hx2 <- dnorm(x2,mean2,sd2)
# create empty plot
plot(x1, hx1, type="n", xlab="", ylab="",
xlim=c(0, 200), ylim=c(-0.003, 0.08),
main="", axes=FALSE)
# draw distributions
lines(x1, hx1)
lines(x2, hx2)
arrows(0, 0, 0, 0.08, xpd = TRUE, length = 0.1)
arrows(0, 0, 200, 0, xpd = TRUE, length = 0.1)
arrows(mean1-sd1, max(hx1), mean1+sd1, max(hx1), length = 0.05, code=3, col="darkorange")
arrows(mean2-sd2, max(hx2), mean2+sd2, max(hx2), length = 0.05, code=3, col="darkorange")
arrows(mean1, 0.05, mean2, 0.05, length = 0.05, code=3, col="darkgreen")
lines(c(mean1,mean1), c(0, 0.08), lty=2, col="blue")
lines(c(mean2, mean2), c(0, 0.08), lty=2, col="blue")
# add text
proby <- TeX("$P(y|x)$")
probtry <- TeX("$P_{\\Omega_{tr}}(y|x)$")
bayesModel <- TeX("$f_B(x)$")
avgModel <- TeX("$\\bar{f}(x)$")
text(65, 0.02,proby,cex=0.9)
text(140, 0.007, probtry, cex=0.9)
text(mean2+15, 0.05, "Small bias", cex=0.9)
text(mean2+sd2+18, max(hx2), "Large variance", cex=0.9)
text(mean1-2, -0.0029, bayesModel, cex=0.9)
text(mean2+2, -0.0029, avgModel, cex=0.9)
text(-4, 0.078, "P")
text(190, -0.0025, "Y")
# ---------------------------------------------------------------------------
############################################################################
# Figure 5.3: The effect of decreasing the variance of probability estimates
# on classification when f > 0.5 and E(PΩT R ) > 0.5.
############################################################################
# ---------------------------------------------------------------------------
# specify means and standard deviations
mean1=720; sd1=15
lb1=700; ub1=720
mean2=220; sd2=40
lb2=200; ub2=220
# simulate data
x1 <- seq(-4,4,length=100)*sd1 + mean1
hx1 <- dnorm(x1,mean1,sd1)
x2 <- seq(-4,4,length=100)*sd2 + mean2
hx2 <- dnorm(x2,mean2,sd2)
# create empty plot
plot(c(x1, x2), c(hx1, hx2), type="n", xlab="", ylab="",
xlim=c(0, 900), ylim=c(-0.003, 0.04),
main="", axes=FALSE)
# draw distributions
lines(x1, hx1)
lines(x2, hx2)
# draw probabilit areas
i <- x1 >= lb1
polygon(c(lb1,x1[i],ub1), c(0,hx1[i],0), col="red")
j <- x2 >= lb2
polygon(c(lb2,x2[j],ub2), c(0,hx2[j],0), col="red")
# make arrows and decision boundary/mean lines
arrows(0, 0, 0, 0.04, xpd = TRUE, length = 0.05)
arrows(500, 0, 500, 0.04, xpd = TRUE, length = 0.05)
arrows(0, 0, 400, 0, xpd = TRUE, length = 0.05)
arrows(500, 0, 900, 0, xpd = TRUE, length = 0.05)
arrows(350, 0.02, 490, 0.02, xpd = TRUE, length = 0.2, lwd=2, col="purple")
arrows(mean1-sd1, max(hx1), mean1+sd1, max(hx1), length = 0.05, code=3, col="darkorange")
arrows(310, 0.01, 250, 0.003, length = 0.05)
arrows(mean2-sd2, max(hx2), mean2+sd2, max(hx2), length = 0.05, code=3, col="darkorange")
arrows(820, 0.01, 740, 0.003, length = 0.05)
lines(c(200,200), c(0, 0.035), lty=2, col="green")
lines(c(700, 700), c(0, 0.035), lty=2, col="green")
lines(c(mean1, mean1), c(0, 0.03), lty=2, col="blue")
lines(c(mean2, mean2), c(0, 0.03), lty=2, col="blue")
# add text
text(-10, 0.038, "P")
text(490, 0.038, "P")
text(0, -0.002, "0.0", cex=0.8)
text(200, -0.002, "0.5", cex=0.8)
text(400, -0.002, "1.0", cex=0.8)
text(500, -0.002, "0.0", cex=0.8)
text(700, -0.002, "0.5", cex=0.8)
text(900, -0.002, "1.0", cex=0.8)
text(415, 0.025, "Decrease Variance", cex = 0.8)
text(200, 0.037, "Decision threshold", cex=0.7)
text(700, 0.037, "Decision threshold", cex=0.7)
text(mean1+5, 0.032, TeX("$E(P_{\\Omega_{TR}})$"), cex=0.8)
text(mean2+5, 0.032, TeX("$E(P_{\\Omega_{TR}})$"), cex=0.8)
text(mean1+sd1+45, max(hx1), TeX("$Var(P_{\\Omega_{TR}})$"), cex=0.8)
text(mean2-sd2-45, max(hx2), TeX("$Var(P_{\\Omega_{TR}})$"), cex=0.8)
text(310, 0.012, TeX("$P(\\bar{g}(x) = g_B(x))$"), cex=0.8)
text(820, 0.012, TeX("$P(\\bar{g}(x) = g_B(x))$"), cex=0.8)
# ---------------------------------------------------------------------------
############################################################################
# Figure 5.4: The effect of increasing the variance of probability estimates
# on classification when f > 0.5 and E(PΩT R ) < 0.5.
############################################################################
# ---------------------------------------------------------------------------
# specify means and standard deviations
mean1=180; sd1=15
lb1=200; ub1=220
mean2=680; sd2=40
lb2=700; ub2=720
# simulate data
x1 <- seq(-4,4,length=100)*sd1 + mean1
hx1 <- dnorm(x1,mean1,sd1)
x2 <- seq(-4,4,length=100)*sd2 + mean2
hx2 <- dnorm(x2,mean2,sd2)
# create empty plot
plot(c(x1, x2), c(hx1, hx2), type="n", xlab="", ylab="",
xlim=c(0, 900), ylim=c(-0.003, 0.04),
main="", axes=FALSE)
# draw distributions
lines(x1, hx1)
lines(x2, hx2)
# draw probabilit areas
i <- x1 >= lb1
polygon(c(lb1,x1[i],ub1), c(0,hx1[i],0), col="red")
j <- x2 >= lb2
polygon(c(lb2,x2[j],ub2), c(0,hx2[j],0), col="red")
# make arrows and decision boundary, mean lines
arrows(0, 0, 0, 0.04, xpd = TRUE, length = 0.05)
arrows(500, 0, 500, 0.04, xpd = TRUE, length = 0.05)
arrows(0, 0, 400, 0, xpd = TRUE, length = 0.05)
arrows(500, 0, 900, 0, xpd = TRUE, length = 0.05)
arrows(350, 0.02, 490, 0.02, xpd = TRUE, length = 0.2, lwd=2, col="purple")
arrows(mean1-sd1, max(hx1), mean1+sd1, max(hx1), length = 0.05, code=3, col="darkorange")
arrows(260, 0.01, 208, 0.003, length = 0.05)
arrows(mean2-sd2, max(hx2), mean2+sd2, max(hx2), length = 0.05, code=3, col="darkorange")
arrows(800, 0.005, 720, 0.003, length = 0.05)
lines(c(200,200), c(0, 0.035), lty=2, col="green")
lines(c(700, 700), c(0, 0.035), lty=2, col="green")
lines(c(180, 180), c(0, 0.03), lty=2, col="blue")
lines(c(680, 680), c(0, 0.03), lty=2, col="blue")
# add text
text(-10, 0.038, "P")
text(490, 0.038, "P")
text(0, -0.002, "0.0", cex=0.8)
text(200, -0.002, "0.5", cex=0.8)
text(400, -0.002, "1.0", cex=0.8)
text(500, -0.002, "0.0", cex=0.8)
text(700, -0.002, "0.5", cex=0.8)
text(900, -0.002, "1.0", cex=0.8)
text(415, 0.025, "Increase Variance", cex=0.8)
text(200, 0.037, "Decision threshold", cex=0.7)
text(700, 0.037, "Decision threshold", cex=0.7)
text(179, 0.032, TeX("$E(P_{\\Omega_{TR}})$"), cex=0.8)
text(679, 0.032, TeX("$E(P_{\\Omega_{TR}})$"), cex=0.8)
text(mean1-sd1-45, max(hx1), TeX("$Var(P_{\\Omega_{TR}})$"), cex=0.8)
text(mean2+sd2+45, max(hx2), TeX("$Var(P_{\\Omega_{TR}})$"), cex=0.8)
text(270, 0.012, TeX("$P(\\bar{g}(x) = g_B(x))$"), cex=0.8)
text(800, 0.006, TeX("$P(\\bar{g}(x) = g_B(x))$"), cex=0.8)
# ---------------------------------------------------------------------------
###########################################################################
# Figure 5.5: Class distributions for a three class classification task:
# Left: The true distribution. Middle: Class distribution over training set
# samples for the first classifier. Right: Class distribution over training
# set samples for the second classifier.
###########################################################################
# ---------------------------------------------------------------------------
# create empty plot
plot(1:12, 1:12, type="n", xlab="", ylab="",
xlim=c(0, 12), ylim=c(-0.04, 1),
main="", axes=FALSE)
# add arrows
arrows(0, 0, 0, 1, xpd = TRUE, length = 0.1)
arrows(0, 0, 12, 0, xpd = TRUE, length = 0.1)
# first distribution
lines(c(0,1), c(0.6, 0.6), col="darkgreen")
lines(c(1,1), c(0.6, 0), col="darkgreen")
lines(c(1,2), c(0.3, 0.3), col="darkorange")
lines(c(2,2), c(0.3, 0), col="darkorange")
lines(c(2,3), c(0.1, 0.1), col="skyblue")
lines(c(3,3), c(0.1, 0), col="skyblue")
# second dist
lines(c(4,4), c(0, 0.1), col="darkgreen")
lines(c(4,5), c(0.1, 0.1), col="darkgreen")
lines(c(5,5), c(0, 0.7), col="darkorange")
lines(c(5,6), c(0.7, 0.7), col="darkorange")
lines(c(6,6), c(0.7, 0), col="darkorange")
lines(c(6,7), c(0.2, 0.2), col="skyblue")
lines(c(7,7), c(0.2, 0), col="skyblue")
# third dist
lines(c(8,8), c(0, 0.2), col="darkgreen")
lines(c(8,9), c(0.2, 0.2), col="darkgreen")
lines(c(9,9), c(0, 0.5), col="darkorange")
lines(c(9,10), c(0.5, 0.5), col="darkorange")
lines(c(10,10), c(0.5, 0), col="darkorange")
lines(c(10,11), c(0.3, 0.3), col="skyblue")
lines(c(11,11), c(0.3, 0), col="skyblue")
# place text
text(-0.17, 0.95, "P")
text(11.6, -0.03, "C")
text(0.5, 0.65, "0.6")
text(1.5, 0.35, "0.3")
text(2.5, 0.15, "0.1")
text(4.5, 0.15, "0.1")
text(5.5, 0.75, "0.7")
text(6.5, 0.25, "0.2")
text(8.5, 0.25, "0.2")
text(9.5, 0.55, "0.5")
text(10.5, 0.35, "0.3")
#classes
text(0.5, -0.03, "1")
text(1.5, -0.03, "2")
text(2.5, -0.03, "3")
text(4.5, -0.03, "1")
text(5.5, -0.03, "2")
text(6.5, -0.03, "3")
text(8.5, -0.03, "1")
text(9.5, -0.03, "2")
text(10.5, -0.03, "3")
# math text
text(1.5, 0.85, TeX("$P(C|x)$"))
text(5.5, 0.85, TeX("$P^1_{\\Omega_{TR}}$"))
text(9.5, 0.85, TeX("$P^2_{\\Omega_{TR}}$"))
# ---------------------------------------------------------------------------
############################################################################
# Figure 5.6: Class distributions for a three class classification task with
# both estimated distributions having equal variance. The true distribution
# is given on the left.
############################################################################
# ---------------------------------------------------------------------------
# create empty plot
plot(1:12, 1:12, type="n", xlab="", ylab="",
xlim=c(0, 12), ylim=c(-0.04, 1),
main="", axes=FALSE)
# add arrows
arrows(0, 0, 0, 1, xpd = TRUE, length = 0.1)
arrows(0, 0, 12, 0, xpd = TRUE, length = 0.1)
# first distribution
lines(c(0,1), c(0.6, 0.6), col="darkgreen")
lines(c(1,1), c(0.6, 0), col="darkgreen")
lines(c(1,2), c(0.3, 0.3), col="darkorange")
lines(c(2,2), c(0.3, 0), col="darkorange")
lines(c(2,3), c(0.1, 0.1), col="skyblue")
lines(c(3,3), c(0.1, 0), col="skyblue")
# second dist
lines(c(4,4), c(0, 0.3), col="darkgreen")
lines(c(4,5), c(0.3, 0.3), col="darkgreen")
lines(c(5,5), c(0, 0.5), col="darkorange")
lines(c(5,6), c(0.5, 0.5), col="darkorange")
lines(c(6,6), c(0.5, 0), col="darkorange")
lines(c(6,7), c(0.2, 0.2), col="skyblue")
lines(c(7,7), c(0.2, 0), col="skyblue")
# third dist
lines(c(8,8), c(0, 0.2), col="darkgreen")
lines(c(8,9), c(0.2, 0.2), col="darkgreen")
lines(c(9,9), c(0, 0.5), col="darkorange")
lines(c(9,10), c(0.5, 0.5), col="darkorange")
lines(c(10,10), c(0.5, 0), col="darkorange")
lines(c(10,11), c(0.3, 0.3), col="skyblue")
lines(c(11,11), c(0.3, 0), col="skyblue")
# place text
text(-0.17, 0.95, "P")
text(11.6, -0.03, "C")
text(0.5, 0.65, "0.6")
text(1.5, 0.35, "0.3")
text(2.5, 0.15, "0.1")
text(4.5, 0.35, "0.3")
text(5.5, 0.55, "0.5")
text(6.5, 0.25, "0.2")
text(8.5, 0.25, "0.2")
text(9.5, 0.55, "0.5")
text(10.5, 0.35, "0.3")
#classes
text(0.5, -0.03, "1")
text(1.5, -0.03, "2")
text(2.5, -0.03, "3")
text(4.5, -0.03, "1")
text(5.5, -0.03, "2")
text(6.5, -0.03, "3")
text(8.5, -0.03, "1")
text(9.5, -0.03, "2")
text(10.5, -0.03, "3")
# math text
text(1.5, 0.75, TeX("$P(C|x)$"))
text(5.5, 0.75, TeX("$P^1_{\\Omega_{TR}}$"))
text(9.5, 0.75, TeX("$P^2_{\\Omega_{TR}}$"))
par(mar=dmar)
1/(1+exp(1.0191))
1/(1+exp(-1.0191))
1/(1+exp(1.091))
1/(1+exp(-2.079 + 1.091))
1/(1+exp(2.079 - 1.091))
exp(1.091)/(1+exp(1.091))
exp(-1.091)/(1+exp(-1.091))
2.977/(1+2.977)
-2.977/(1-2.977)
exp(-1.091)
exp(-1.091)/(1+exp(-1.091))
exp(1.091)
odds20 <- -3.654+20*0.157
probs <- odds20/(1+odds20)
probs
sig <- exp(odds20)
probs <- sig/(1+sig)
probs
odds20 <- -3.654+21*0.157
sig <- exp(odds20)
probs <- sig/(1+sig)
probs
change1 <- 0.411686-3742563
change1
change1 <- 0.411686-0.3742563
change1
odds20 <- -3.654+0.157
odds20 <- -3.654
sig <- exp(odds20)
probs <- sig/(1+sig)
probs
odds20 <- -3.654+0.157
sig <- exp(odds20)
probs <- sig/(1+sig)
probs
change2 <- 0.02939771-0.02523413
change2
setwd("/Users/arnupretorius/Google Drive/University/Masters/Thesis Code/Chapter 7")
list.of.packages <- c("plyr","dplyr","latex2exp", "mlbench", "ggplot2", "caret", "doSNOW", "lattice",
"obliqueRF", "MASS", "stargazer", "rotationForest", "randomForest",
"scmamp", "surv2sampleComp", "ElemStatLearn", "hmeasure")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# load required packages
load <- lapply(list.of.packages, require, character.only = TRUE)
data <- read.csv("RFComparisonsData.csv")
evalMeth <- data %>% select(paper_title, evaluation)
evalMeth <- unique(evalMeth)
evalMeth <- as.data.frame(evalMeth %>% count(evaluation))
evalMeth <- evalMeth[order(evalMeth$n, decreasing = TRUE),]
data <- read.csv("RFComparisonsData.csv")
evalMeth <- data %>% select(paper_title, evaluation)
evalMeth <- unique(evalMeth)
evalMeth <- as.data.frame(evalMeth %>% count(evaluation))
evalMeth <- evalMeth[order(evalMeth$n, decreasing = TRUE),]
list.of.packages <- c("plyr","dplyr","latex2exp", "mlbench", "ggplot2", "caret", "doSNOW", "lattice",
"obliqueRF", "stargazer", "rotationForest", "randomForest",
"scmamp", "surv2sampleComp", "ElemStatLearn", "hmeasure")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# load required packages
load <- lapply(list.of.packages, require, character.only = TRUE)
data <- read.csv("RFComparisonsData.csv")
evalMeth <- data %>% select(paper_title, evaluation)
evalMeth <- unique(evalMeth)
evalMeth <- as.data.frame(evalMeth %>% count(evaluation))
evalMeth <- evalMeth[order(evalMeth$n, decreasing = TRUE),]
list.of.packages <- c("dplyr","latex2exp", "mlbench", "ggplot2", "caret", "doSNOW", "lattice",
"obliqueRF", "stargazer", "rotationForest", "randomForest",
"scmamp", "surv2sampleComp", "ElemStatLearn", "hmeasure")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# load required packages
load <- lapply(list.of.packages, require, character.only = TRUE)
data <- read.csv("RFComparisonsData.csv")
evalMeth <- data %>% select(paper_title, evaluation)
evalMeth <- unique(evalMeth)
evalMeth <- as.data.frame(evalMeth %>% count(evaluation))
evalMeth <- evalMeth[order(evalMeth$n, decreasing = TRUE),]
list.of.packages <- c("MASS","dplyr","latex2exp", "mlbench", "ggplot2", "caret", "doSNOW", "lattice",
"obliqueRF", "stargazer", "rotationForest", "randomForest",
"scmamp", "surv2sampleComp", "ElemStatLearn", "hmeasure")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# load required packages
load <- lapply(list.of.packages, require, character.only = TRUE)
data <- read.csv("RFComparisonsData.csv")
evalMeth <- data %>% select(paper_title, evaluation)
evalMeth <- unique(evalMeth)
evalMeth <- as.data.frame(evalMeth %>% count(evaluation))
evalMeth <- evalMeth[order(evalMeth$n, decreasing = TRUE),]
data <- read.csv("RFComparisonsData.csv")
# remove observations that are not "reliable"
data <- filter(data, data$evaluation != "OOB")
data <- filter(data, data$evaluation != "1 run")
data <- filter(data, data$evaluation != "3-fold cv")
data <- filter(data, !is.na(data$evaluation))
# split between allround situations and high-dimensional situations
allround <- filter(data, data$situation == "allround")
HD <- filter(data, data$situation == "HD" | data$situation == "select-HD")
lopHD <- arrange(HD, dataset) %>% select(dataset, method, error)
lopHD <- as.data.frame(summarise(group_by(lopHD, dataset, method), mean(error)))
lopHD$acc <- round(100-lopHD$`mean(error)`,3)
# create compare matrix
lopHDSplit <- split(lopHD, factor(lopHD$dataset))
dsets <- factor(unique(lopHD$dataset))
methods <- factor(unique(lopHD$method))
compareMat <- matrix(0, nrow=length(dsets), ncol=length(methods))
rownames(compareMat) <- dsets
colnames(compareMat) <- methods
for(i in 1:length(lopHDSplit)){
for(j in 1:nrow(lopHDSplit[[i]])){
compareMat[i, which(methods == as.character(lopHDSplit[[i]]$method[j]))] <- lopHDSplit[[i]]$acc[j]
}
}
#remove rare data sets
removeRowIndex <- apply(compareMat, 1, function(x){
ifelse(length(which(x != 0)) < 3, 1, 0)
})
#remove algorithms fitted to only a very small number of data sets
removeColIndex <- apply(compareMat, 2, function(x){
ifelse(length(which(x != 0)) < 5, 1, 0)
})
removeRowIndex <- which(removeRowIndex == 1) #(none found)
removeColIndex <- which(removeColIndex == 1)
compareMat <- compareMat[, -removeColIndex]
# compute nomial ranks
rankMat <- apply(compareMat, 1, function(x){
index <- which(x != 0)
rankVec <- rank(-x[index], ties.method = "average")
x[index] <- rankVec/length(rankVec)
x
})
rankMat <- t(rankMat)
# adjust for number of datasets used
rankMat <- apply(rankMat, 2, function(x){
prop <- length(which(x == 0))/length(x)
if(prop == 0){
prop <- 1/(length(x)+1)
}
x*prop
})
# compute average rank per method
avgRanks <- apply(rankMat, 2, function(x){
index <- which(x != 0)
mean(x[index])
})
# scale ranks
range2 = length(avgRanks) - 1
avgRanksStand = (avgRanks*range2) + 1
# sorted ranks
sortAvgRanks <- sort(avgRanksStand)
# plot sorted ranks
plotData <- data.frame(rank=sortAvgRanks, names=names(sortAvgRanks))
ggplot(plotData, aes(x=names, y=rank)) +
geom_bar(stat="identity", fill="orange") + ylab("Adjusted Rank") +
xlab("Algorithm") +
scale_x_discrete(limits=names(sortAvgRanks))+ theme_bw()+
theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))+
ggtitle("Hi-dimensional algorithms")
ggplot(plotData, aes(x=names, y=rank)) +
geom_bar(stat="identity", fill="orange") + ylab("Adjusted Rank") +
xlab("Algorithm") +
scale_x_discrete(limits=names(sortAvgRanks))+ theme_bw()+
theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))+
ggtitle("High-dimensional algorithms")
