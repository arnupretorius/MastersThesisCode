allDSCount <- rep(0, length(allDS))
dsSplit <- split(ds, ds$paper_title)
for(i in 1:length(dsSplit)){
dsPerPaper <-  unique(dsSplit[[i]][,2])
for(j in 1:length(dsPerPaper)){
index <- which(allDS == dsPerPaper[j])
allDSCount[index] <- allDSCount[index] + 1
}
}
dsFrame <- data.frame(dataset=allDS, freqUsed=allDSCount)
dsFrame <- dsFrame[order(dsFrame$freqUsed, decreasing = TRUE),]
# get dataset characteristics
dataChar <- data %>% select(dataset, dataset_size, num_inputs, classes) %>% distinct()
dataChar <- merge(dsFrame, dataChar)
dataChar <- dataChar[order(dataChar$freqUsed, decreasing = TRUE),]
labels <- c("(208,60,2)", "(699,9,2)", "(214,9,6)",
"(768,8,2)", "(846,18,4)",
"(351,34,2)", "(20000,16,26)",
"(62,4026,3)", "(1000,20,2)",
"(435,16,2)")
# plot data sets used
ggplot(dsFrame[1:10,], aes(x=dataset, y=freqUsed)) + geom_bar(stat="identity", fill="skyblue") +
xlab("Benchmark data set (#obs, #inputs, #classes)") +
ylab("#Papers")+
scale_x_discrete(limits=unique(dataChar$dataset)[1:10])+ theme_bw()+
scale_y_continuous(breaks = seq(0, 17, by = 2))+
theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))+
geom_text(aes(label=labels), vjust=3, color="white", size=3,
hjust=1, angle=60)
# plot variatability of RF on above data sets
rfDataSets <- factor(unique(dataChar$dataset)[1:10])
keepIndex <- NULL
count <- 1
for(i in 1:nrow(data)){
if(data[i,]$method == "rf" && data[i,]$dataset %in% rfDataSets){
keepIndex[count] <- i
count <- count + 1
}
}
rfCompData <- data[keepIndex,] %>% select(paper_title, dataset, method, error)
rfCompData <- rfCompData[order(rfCompData$dataset),]
# remove other lymphoma
lympRemove <- NULL
count <- 1
for(i in 1:nrow(rfCompData)){
if(rfCompData[i,]$dataset == "lymphoma" && rfCompData[i,]$error > 3){
lympRemove[count] <- i
count <- count + 1
}
}
rfCompData <- rfCompData[-lympRemove,]
rfCompData <- rfCompData[-101, ] # remove gross outlier in: On extreme pruning (possibly reported error instead of acc)
# plot errors
ggplot(rfCompData, aes(y=error, x=dataset)) + geom_boxplot(fill="skyblue", outlier.colour = "red")+
theme_bw() + ylab("Reported error rates") + xlab("Benchmark data set")+
scale_x_discrete(limits=unique(dataChar$dataset)[1:10])+
theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))
# outlier breast cancer: On extreme pruning (Possibly not the same breast cancer dataset)
# outlier glass: Tripoli et al. paper
# outlier sonar: On extreme pruning (possibly reported error instead of acc)
# plot evaluation method used
evalsData <- data %>% select(paper_title, comparison) %>% distinct()
lims <- unique(evalsData$comparison)[c(1,2,4,3,5,6,7,8)]
ggplot(evalsData, aes(x=comparison)) + geom_bar(fill="darkgreen") +
xlab("Comparison method") + ylab("#Papers")+
theme_bw()+
scale_x_discrete(limits=lims)+
scale_y_continuous(breaks = seq(0, 20, by = 2))+
theme(axis.text.x = element_text(angle = 10, vjust = 1, hjust = 1))
list.files()
setwd("/Users/arnupretorius/Google Drive/University/Masters/Thesis Code/Chapter 7")
list.files()
data <- read.csv("RFComparisonsData.csv")
# Investigation of RF variatibiliy across different papers #
#########################################################
# compute top used data sets across papers
ds <- data %>% select(paper_title, dataset)
allDS <- unique(ds$dataset)
allDSCount <- rep(0, length(allDS))
dsSplit <- split(ds, ds$paper_title)
for(i in 1:length(dsSplit)){
dsPerPaper <-  unique(dsSplit[[i]][,2])
for(j in 1:length(dsPerPaper)){
index <- which(allDS == dsPerPaper[j])
allDSCount[index] <- allDSCount[index] + 1
}
}
dsFrame <- data.frame(dataset=allDS, freqUsed=allDSCount)
dsFrame <- dsFrame[order(dsFrame$freqUsed, decreasing = TRUE),]
# get dataset characteristics
dataChar <- data %>% select(dataset, dataset_size, num_inputs, classes) %>% distinct()
dataChar <- merge(dsFrame, dataChar)
dataChar <- dataChar[order(dataChar$freqUsed, decreasing = TRUE),]
labels <- c("(208,60,2)", "(699,9,2)", "(214,9,6)",
"(768,8,2)", "(846,18,4)",
"(351,34,2)", "(20000,16,26)",
"(62,4026,3)", "(1000,20,2)",
"(435,16,2)")
# plot data sets used
ggplot(dsFrame[1:10,], aes(x=dataset, y=freqUsed)) + geom_bar(stat="identity", fill="skyblue") +
xlab("Benchmark data set (#obs, #inputs, #classes)") +
ylab("#Papers")+
scale_x_discrete(limits=unique(dataChar$dataset)[1:10])+ theme_bw()+
scale_y_continuous(breaks = seq(0, 17, by = 2))+
theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))+
geom_text(aes(label=labels), vjust=3, color="white", size=3,
hjust=1, angle=60)
# plot variatability of RF on above data sets
rfDataSets <- factor(unique(dataChar$dataset)[1:10])
keepIndex <- NULL
count <- 1
for(i in 1:nrow(data)){
if(data[i,]$method == "rf" && data[i,]$dataset %in% rfDataSets){
keepIndex[count] <- i
count <- count + 1
}
}
rfCompData <- data[keepIndex,] %>% select(paper_title, dataset, method, error)
rfCompData <- rfCompData[order(rfCompData$dataset),]
# remove other lymphoma
lympRemove <- NULL
count <- 1
for(i in 1:nrow(rfCompData)){
if(rfCompData[i,]$dataset == "lymphoma" && rfCompData[i,]$error > 3){
lympRemove[count] <- i
count <- count + 1
}
}
rfCompData <- rfCompData[-lympRemove,]
rfCompData <- rfCompData[-101, ] # remove gross outlier in: On extreme pruning (possibly reported error instead of acc)
# plot errors
ggplot(rfCompData, aes(y=error, x=dataset)) + geom_boxplot(fill="skyblue", outlier.colour = "red")+
theme_bw() + ylab("Reported error rates") + xlab("Benchmark data set")+
scale_x_discrete(limits=unique(dataChar$dataset)[1:10])+
theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))
# outlier breast cancer: On extreme pruning (Possibly not the same breast cancer dataset)
# outlier glass: Tripoli et al. paper
# outlier sonar: On extreme pruning (possibly reported error instead of acc)
# plot evaluation method used
evalsData <- data %>% select(paper_title, comparison) %>% distinct()
lims <- unique(evalsData$comparison)[c(1,2,4,3,5,6,7,8)]
ggplot(evalsData, aes(x=comparison)) + geom_bar(fill="darkgreen") +
xlab("Comparison method") + ylab("#Papers")+
theme_bw()+
scale_x_discrete(limits=lims)+
scale_y_continuous(breaks = seq(0, 20, by = 2))+
theme(axis.text.x = element_text(angle = 10, vjust = 1, hjust = 1))
list.of.packages <- c("dplyr","latex2exp", "mlbench", "ggplot2", "caret", "doSNOW", "lattice",
"obliqueRF", "MASS", "stargazer", "rotationForest", "randomForest",
"scmamp", "surv2sampleComp", "ElemStatLearn", "hmeasure")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# load required packages
load <- lapply(list.of.packages, require, character.only = TRUE)
# required packages from bioconductor for scmamp package
source("https://bioconductor.org/biocLite.R")
biocLite("graph")
biocLite("graph")
biocLite("Rgraphviz")
# download and load random rotation forests package
if("RRotF" %in% installed.packages()[,"Package"] == FALSE){
library(devtools)
install_github("arnupretorius/RRotF")
}
library(RRotF)
data <- read.csv("RFComparisonsData.csv")
evalMeth <- data %>% select(paper_title, evaluation)
evalMeth <- unique(evalMeth)
evalMeth <- as.data.frame(evalMeth %>% count(evaluation))
evalMeth <- evalMeth[order(evalMeth$n, decreasing = TRUE),]
# mark which estimation methods are not "reliable"
notIndex <- c(4, 9, 22, 27)
grp <- rep("Reliable", 27)
grp[notIndex] <- "Not reliable"
evalMeth$grp <- grp
# plot estimation methods used
ggplot(evalMeth, aes(x=evaluation, y=n, fill=grp)) + geom_bar(stat="identity") +
scale_x_discrete(limits=evalMeth$evaluation) +
scale_fill_manual(name="" ,values=c("darkgreen", "skyblue")) +
theme_bw() + ylab("#Papers") + xlab("Estimation method") +
theme(legend.position = c(0.9,0.7), axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1))
data <- read.csv("RFComparisonsData.csv")
# Investigation of RF variatibiliy across different papers #
#########################################################
# compute top used data sets across papers
ds <- data %>% select(paper_title, dataset)
allDS <- unique(ds$dataset)
allDSCount <- rep(0, length(allDS))
dsSplit <- split(ds, ds$paper_title)
for(i in 1:length(dsSplit)){
dsPerPaper <-  unique(dsSplit[[i]][,2])
for(j in 1:length(dsPerPaper)){
index <- which(allDS == dsPerPaper[j])
allDSCount[index] <- allDSCount[index] + 1
}
}
dsFrame <- data.frame(dataset=allDS, freqUsed=allDSCount)
dsFrame <- dsFrame[order(dsFrame$freqUsed, decreasing = TRUE),]
# get dataset characteristics
dataChar <- data %>% select(dataset, dataset_size, num_inputs, classes) %>% distinct()
dataChar <- merge(dsFrame, dataChar)
dataChar <- dataChar[order(dataChar$freqUsed, decreasing = TRUE),]
labels <- c("(208,60,2)", "(699,9,2)", "(214,9,6)",
"(768,8,2)", "(846,18,4)",
"(351,34,2)", "(20000,16,26)",
"(62,4026,3)", "(1000,20,2)",
"(435,16,2)")
# plot data sets used
ggplot(dsFrame[1:10,], aes(x=dataset, y=freqUsed)) + geom_bar(stat="identity", fill="skyblue") +
xlab("Benchmark data set (#obs, #inputs, #classes)") +
ylab("#Papers")+
scale_x_discrete(limits=unique(dataChar$dataset)[1:10])+ theme_bw()+
scale_y_continuous(breaks = seq(0, 17, by = 2))+
theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))+
geom_text(aes(label=labels), vjust=3, color="white", size=3,
hjust=1, angle=60)
data <- read.csv("RFComparisonsData.csv")
# Investigation of RF variatibiliy across different papers #
#########################################################
# compute top used data sets across papers
ds <- data %>% select(paper_title, dataset)
allDS <- unique(ds$dataset)
allDSCount <- rep(0, length(allDS))
dsSplit <- split(ds, ds$paper_title)
for(i in 1:length(dsSplit)){
dsPerPaper <-  unique(dsSplit[[i]][,2])
for(j in 1:length(dsPerPaper)){
index <- which(allDS == dsPerPaper[j])
allDSCount[index] <- allDSCount[index] + 1
}
}
dsFrame <- data.frame(dataset=allDS, freqUsed=allDSCount)
dsFrame <- dsFrame[order(dsFrame$freqUsed, decreasing = TRUE),]
# get dataset characteristics
dataChar <- data %>% select(dataset, dataset_size, num_inputs, classes) %>% distinct()
dataChar <- merge(dsFrame, dataChar)
dataChar <- dataChar[order(dataChar$freqUsed, decreasing = TRUE),]
# plot variatability of RF on above data sets
rfDataSets <- factor(unique(dataChar$dataset)[1:10])
keepIndex <- NULL
count <- 1
for(i in 1:nrow(data)){
if(data[i,]$method == "rf" && data[i,]$dataset %in% rfDataSets){
keepIndex[count] <- i
count <- count + 1
}
}
rfCompData <- data[keepIndex,] %>% select(paper_title, dataset, method, error)
rfCompData <- rfCompData[order(rfCompData$dataset),]
# remove other lymphoma
lympRemove <- NULL
count <- 1
for(i in 1:nrow(rfCompData)){
if(rfCompData[i,]$dataset == "lymphoma" && rfCompData[i,]$error > 3){
lympRemove[count] <- i
count <- count + 1
}
}
rfCompData <- rfCompData[-lympRemove,]
rfCompData <- rfCompData[-101, ] # remove gross outlier in: On extreme pruning (possibly reported error instead of acc)
# plot errors
ggplot(rfCompData, aes(y=error, x=dataset)) + geom_boxplot(fill="skyblue", outlier.colour = "red")+
theme_bw() + ylab("Reported error rates") + xlab("Benchmark data set")+
scale_x_discrete(limits=unique(dataChar$dataset)[1:10])+
theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))
evalsData <- data %>% select(paper_title, comparison) %>% distinct()
lims <- unique(evalsData$comparison)[c(1,2,4,3,5,6,7,8)]
ggplot(evalsData, aes(x=comparison)) + geom_bar(fill="darkgreen") +
xlab("Comparison method") + ylab("#Papers")+
theme_bw()+
scale_x_discrete(limits=lims)+
scale_y_continuous(breaks = seq(0, 20, by = 2))+
theme(axis.text.x = element_text(angle = 10, vjust = 1, hjust = 1))
dataSplit <- split(data, factor(data$paper_title))
pvals <- NULL
checkPhTest <- NULL
phTests <- list()
# for each paper build a compare matrix and compute the Ivan-Davenport test p-value
for(k in 1:length(dataSplit)){
lop <- arrange(dataSplit[[k]], dataset) %>% select(dataset, method, error)
lop <- as.data.frame(summarise(group_by(lop, dataset, method), mean(error)))
lop$acc <- round(100-lop$`mean(error)`,3)
# create compare matrix
lopSplit <- split(lop, factor(lop$dataset))
dsets <- unique(lop$dataset)
methods <- unique(lop$method)
compareMat <- matrix(0, nrow=length(dsets), ncol=length(methods))
rownames(compareMat) <- dsets
colnames(compareMat) <- methods
for(i in 1:length(lopSplit)){
for(j in 1:nrow(lopSplit[[i]])){
compareMat[i, which(methods == as.character(lopSplit[[i]]$method[j]))] <- lopSplit[[i]]$acc[j]
}
}
pvals[k] <- round(imanDavenportTest(compareMat)[[3]], 3)
if(!is.na(pvals[k]) && pvals[k] < 0.05 && "rf" %in% colnames(compareMat)){
phTests[[k]] <- postHocTest(compareMat, test = "friedman",
control = "rf", correct = "finner", alpha = 0.05)
checkPhTest[k] <- ifelse(length(which(phTests[[k]]$corrected.pval < 0.05)) == 0, TRUE, FALSE)
}
}
# paper where no significant result was found
omnibusFailed <- which(pvals > 0.05)
PHvsRFFailed <- which(checkPhTest)
nonSigResult <- c(omnibusFailed, PHvsRFFailed)
# plot pvals
pvals[PHvsRFFailed] <- 0.05
grp <- rep("Omnibus: Iman-Davenport", length(pvals))
grp[PHvsRFFailed] <- "Post-hoc: Finner (Forest-RI as control)"
grp <- factor(grp)
pvalData <- data.frame(pv = pvals[-29], Test=grp[-29])
ggplot(pvalData, aes(x=1:nrow(pvalData), y=pv, fill=Test)) + geom_bar(stat="identity") +
theme_bw() + xlab("'Papers' (no names given)") +
ylab("p-value") + geom_hline(yintercept = 0.05, col="red", linetype="dashed") +
scale_fill_manual(values=c("darkgreen", "skyblue"))+
scale_x_continuous(breaks = seq(from=1,to=34, by=1))+
annotate("text", x=0.5, y=0.09, label = "alpha==0.05 ",parse = TRUE)+
theme(legend.position=c(0.2,0.8))
data <- read.csv("RFComparisonsData.csv")
# remove observations that are not "reliable"
data <- filter(data, data$evaluation != "OOB")
data <- filter(data, data$evaluation != "1 run")
data <- filter(data, data$evaluation != "3-fold cv")
data <- filter(data, !is.na(data$evaluation))
# split between allround situations and high-dimensional situations
allround <- filter(data, data$situation == "allround")
HD <- filter(data, data$situation == "HD" | data$situation == "select-HD")
# ALLROUND methods
lop <- arrange(allround, dataset) %>% select(dataset, method, error)
lop <- as.data.frame(summarise(group_by(lop, dataset, method), mean(error)))
lop$acc <- round(100-lop$`mean(error)`,3)
# create compare matrix
lopSplit <- split(lop, factor(lop$dataset))
dsets <- unique(lop$dataset)
methods <- unique(lop$method)
compareMat <- matrix(0, nrow=length(dsets), ncol=length(methods))
rownames(compareMat) <- dsets
colnames(compareMat) <- methods
for(i in 1:length(lopSplit)){
for(j in 1:nrow(lopSplit[[i]])){
compareMat[i, which(methods == as.character(lopSplit[[i]]$method[j]))] <- lopSplit[[i]]$acc[j]
}
}
#remove rare data sets
removeRowIndex <- apply(compareMat, 1, function(x){
ifelse(length(which(x != 0)) < 3, 1, 0)
})
removeColIndex <- apply(compareMat, 2, function(x){
ifelse(length(which(x != 0)) < 10, 1, 0)
})
removeRowIndex <- which(removeRowIndex == 1)
removeColIndex <- which(removeColIndex == 1)
compareMat <- compareMat[-removeRowIndex,-removeColIndex]
# compute nomial ranks
rankMat <- apply(compareMat, 1, function(x){
index <- which(x != 0)
rankVec <- rank(-x[index], ties.method = "average")
x[index] <- rankVec/length(rankVec)
x
})
rankMat <- t(rankMat)
# adjust for number of datasets used
rankMat <- apply(rankMat, 2, function(x){
prop <- length(which(x == 0))/length(x)
x*prop
})
# compute average rank per method
avgRanks <- apply(rankMat, 2, function(x){
index <- which(x != 0)
mean(x[index])
})
# scale ranks
range2 = length(avgRanks) - 1
avgRanksStand = (avgRanks*range2) + 1
# sorted ranks
sortAvgRanks <- sort(avgRanksStand)
# plot sorted ranks
plotData <- data.frame(rank=sortAvgRanks, names=names(sortAvgRanks))
ggplot(plotData, aes(x=names, y=rank)) +
geom_bar(stat="identity", fill="orange") + ylab("Adjusted Rank") +
xlab("Algorithm") +
scale_x_discrete(limits=names(sortAvgRanks))+ theme_bw()+
theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))+
ggtitle("All-round algorithms")
top5Algs <- names(sortAvgRanks)[1:5]
# find data sets associated with each rf
rfsAlgs <- top5Algs
algsDatasets <- list()
algsDatLen <- NULL
temp <- NULL
for(i in 1:length(rfsAlgs)){
for(j in 1:length(lopSplit)){
if(rfsAlgs[i] %in% lopSplit[[j]][,2]){
temp <- c(temp, as.character(lopSplit[[j]][1,1]))
}
}
algsDatasets[[i]] <- temp
algsDatLen[i] <- length(temp)
temp <- NULL
}
# (all algs come from same article => same 24 datasets)
# define reduced comare matrix
datasets <- algsDatasets[[2]]
cAlgs <- top5Algs
rowIndex <- sapply(rownames(compareMat) , function(x) x %in% datasets)
colIndex <- sapply(colnames(compareMat), function(x) x %in% cAlgs)
rCompareMat <- compareMat[rowIndex, colIndex]
# plot densities
plotDensities(rCompareMat) + xlab("Accuracy") + theme_bw() + theme(legend.position=c(0.2, 0.7))
# perform Iman-Devenport test
imanDavenportTest(rCompareMat)
# (significant dfference found => perform post-hoc test)
# perform Shaffer's static test
pvalsShaffer <- postHocTest(rCompareMat, test = "friedman", use.rank=TRUE, correct="shaffer")
# plot p-values and hypothesis tests
# Shaffer
plotPvalues(pvalsShaffer$corrected.pval) + ggtitle("Shaffer's static")
drawAlgorithmGraph(pvalsShaffer$corrected.pval, pvalsShaffer$summary, font.size = 5)
lopHD <- arrange(HD, dataset) %>% select(dataset, method, error)
lopHD <- as.data.frame(summarise(group_by(lopHD, dataset, method), mean(error)))
lopHD$acc <- round(100-lopHD$`mean(error)`,3)
# create compare matrix
lopHDSplit <- split(lopHD, factor(lopHD$dataset))
dsets <- factor(unique(lopHD$dataset))
methods <- factor(unique(lopHD$method))
compareMat <- matrix(0, nrow=length(dsets), ncol=length(methods))
rownames(compareMat) <- dsets
colnames(compareMat) <- methods
for(i in 1:length(lopHDSplit)){
for(j in 1:nrow(lopHDSplit[[i]])){
compareMat[i, which(methods == as.character(lopHDSplit[[i]]$method[j]))] <- lopHDSplit[[i]]$acc[j]
}
}
#remove rare data sets
removeRowIndex <- apply(compareMat, 1, function(x){
ifelse(length(which(x != 0)) < 3, 1, 0)
})
#remove algorithms fitted to only a very small number of data sets
removeColIndex <- apply(compareMat, 2, function(x){
ifelse(length(which(x != 0)) < 5, 1, 0)
})
removeRowIndex <- which(removeRowIndex == 1) #(none found)
removeColIndex <- which(removeColIndex == 1)
compareMat <- compareMat[, -removeColIndex]
# compute nomial ranks
rankMat <- apply(compareMat, 1, function(x){
index <- which(x != 0)
rankVec <- rank(-x[index], ties.method = "average")
x[index] <- rankVec/length(rankVec)
x
})
rankMat <- t(rankMat)
# adjust for number of datasets used
rankMat <- apply(rankMat, 2, function(x){
prop <- length(which(x == 0))/length(x)
if(prop == 0){
prop <- 1/(length(x)+1)
}
x*prop
})
# compute average rank per method
avgRanks <- apply(rankMat, 2, function(x){
index <- which(x != 0)
mean(x[index])
})
# scale ranks
range2 = length(avgRanks) - 1
avgRanksStand = (avgRanks*range2) + 1
# sorted ranks
sortAvgRanks <- sort(avgRanksStand)
# plot sorted ranks
plotData <- data.frame(rank=sortAvgRanks, names=names(sortAvgRanks))
ggplot(plotData, aes(x=names, y=rank)) +
geom_bar(stat="identity", fill="orange") + ylab("Adjusted Rank") +
xlab("Algorithm") +
scale_x_discrete(limits=names(sortAvgRanks))+ theme_bw()+
theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))+
ggtitle("Hi-dimensional algorithms")
# choose top 5 algorithms
top5Algs <- names(sortAvgRanks)[1:5]
# find data sets associated with each rf
rfsAlgs <- top5Algs
algsDatasets <- list()
algsDatLen <- NULL
temp <- NULL
for(i in 1:length(rfsAlgs)){
for(j in 1:length(lopHDSplit)){
if(rfsAlgs[i] %in% lopHDSplit[[j]][,2]){
temp <- c(temp, as.character(lopHDSplit[[j]][1,1]))
}
}
algsDatasets[[i]] <- temp
algsDatLen[i] <- length(temp)
temp <- NULL
}
# define reduced comare matrix
datasets <- intersect(intersect(algsDatasets[[1]], algsDatasets[[2]]), algsDatasets[[3]])
cAlgs <- top5Algs
rowIndex <- sapply(rownames(compareMat) , function(x) x %in% datasets)
colIndex <- sapply(colnames(compareMat), function(x) x %in% cAlgs)
rCompareMat <- compareMat[rowIndex, colIndex]
# plot densities
plotDensities(rCompareMat) + xlab("Accuracy") + theme_bw() + theme(legend.position=c(0.2, 0.7))
# perform Iman-Devenport test
imanDavenportTest(rCompareMat)
# (significant dfference found => perform post-hoc test)
# perform Shaffer's static test
pvalsShaffer <- postHocTest(rCompareMat, test = "friedman", use.rank=TRUE, correct="shaffer")
# plot p-values and hypothesis tests
# Shaffer
plotPvalues(pvalsShaffer$corrected.pval) + ggtitle("Shaffer's static")
drawAlgorithmGraph(pvalsShaffer$corrected.pval, pvalsShaffer$summary, font.size = 5)
